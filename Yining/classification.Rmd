---
title: 'STAT 542: Final Project'
author: "Karthik Vasu (kvasu2), Yining Lu (yining13)"
date: 'Due: 05/05/2022'
theme: readable
output:
  pdf_document:
    toc: yes
    toc_depth: 2
bibliography: references.bib
header-includes:
  - \usepackage{amsmath}
---

```{r, echo=FALSE}
train = read.csv("dataset/fashion-mnist_train.csv")
test = read.csv("dataset/fashion-mnist_test.csv")

Xtrain = train[1:6000,2:785]
Ytrain = train[1:6000,1]

Xtest = test[1:1000,2:785]
Ytest = test[1:1000,1]
```

# Summary Statistics

Data table

```{r, echo=FALSE}
table(Ytrain)
table(Ytest)
```

# Pre processing

```{r}
X.train.sca = scale(Xtrain)
```

# Multi-class Classification Model

- We have learned several different models that can perform multi-class classification. In this question, choose two of them and properly tune these models. Report the overall classification error and provide sufficient information (table/figure and descriptions) to demonstrate the model fitting results.

- Many models we learned are only for binary classifications. Try to extend one of them to handle multi-class problems. You can either search the existing literature to implement the method (or use their packages) or write your own code based on an ideal you have. Clearly describe your method framework, demonstrate the results and compare that with the two previous models.

```{r}
library(class)
# 5-fold cross validation
nfold = 5
infold = sample(rep(1:nfold, length.out=nrow(Xtrain)))
allk = c(1:10)
errMatrix = matrix(NA, length(allk), nfold)
for (i in 1:nfold) {
  for (k in 1:length(allk)) {
    class_knn = knn(Xtrain[infold!=i,], Xtrain[infold==i,], Ytrain[infold!=i], k=k)
    errMatrix[k,i] = mean(class_knn != Ytrain[infold==i])
  }
}
```

```{r}
plot(allk, apply(errMatrix,1,mean), col="red")
best_k = allk[which.min(apply(errMatrix,1,mean))]
Ytest_pred = knn(Xtrain, Xtest, Ytrain, k=best_k)
# confusion matrix
confuse = table(Ytest, Ytest_pred)
# mis-classification rate
misclass_rate = rep(NA, 10)
number = rep(NA, 10)
for (i in 1:10) {
  misclass_rate[i] = 1 - confuse[i,i]/sum(confuse[,i])
}
class = 0:9
misclass = cbind(class, misclass_rate)
# overall mis-classification rate
overall_misclass = mean(Ytest_pred != Ytest)
confuse
misclass
overall_misclass
```

```{r}
# fit logistic regression
#library(glmnet)
#lambda = seq(0.05, 1, by=0.05)
#class_logi = cv.glmnet(as.matrix(Xtrain), Ytrain, alpha=1, lambda=lambda, family="multinomial")
#plot(class_logi)
#Ytest_pred = predict(class_logi, as.matrix(Xtest), s="lambda.min", type="class")
```

```{r}
# fit LDA
library(MASS)
class_lda = lda(Xtrain, Ytrain)
Ytest_pred = predict(class_lda, Xtest)$class
```

```{r}
# confusion matrix
confuse = table(Ytest, Ytest_pred)
# mis-classification rate
misclass_rate = rep(NA, 10)
number = rep(NA, 10)
for (i in 1:10) {
  misclass_rate[i] = 1 - confuse[i,i]/sum(confuse[,i])
}
class = 0:9
misclass = cbind(class, misclass_rate)
# overall mis-classification rate
overall_misclass = mean(Ytest_pred != Ytest)
confuse
misclass
overall_misclass
```