---
title: 'STAT 542: Final Project'
author: "Karthik Vasu (kvasu2), Yining Lu (yining13)"
date: 'Due: 05/05/2022'
theme: readable
output:
  pdf_document:
    toc: yes
    toc_depth: 2
bibliography: references.bib
header-includes:
  - \usepackage{amsmath}
---

# Literature review

The Fashion-MNIST data set has been created by researchers at Zalando for the purposes of benchmarking ML algorithms. It consists of 70000 grayscale images of dimension 28*28. These are images of clothing articles like T-shirt, Trouser, Pullover etc with 60000 training samples and 10000 testing samples. The purpose of this data set is to provide a more challenging classifying compared to the original MNIST data. There are algorithms which 99% accuracy on this making it too easy for modern algorithms.

The best accuracy we found was by a GitHub user named [_Andy Brock_](https://github.com/ajbrock) who was able to achieve an accuracy of 96.7% using wide residual networks. A lot of people have implemented algorithms with high accuracy. They can be for on [_Zalando Research's GitHub page_](https://github.com/zalandoresearch/fashion-mnist).

@xiao2017/online test out a variety of classifiers including Decision Tree ,Gradient Boosting, K Neighbors, Linear SVC, Logistic Regression and many more. They achieve the best result using the SVC classifier with C=10 and the rbf kernel. The testing accuracy for this algorithm is 89.7% on the fashion data set and 97.3% on the original MNIST data. Gradient boosting performs well with testing accuracy at 88% and 96.9% respectively. This is achieved for n_estimators=100 and max_depth=10.

%%one more paper to be added%%


```{r, echo=FALSE}
train = read.csv("fashion-mnist_train.csv")
test = read.csv("fashion-mnist_test.csv")

Xtrain = train[,2:785]
Ytrain = train[,1]

Xtest = test[,2:785]
Ytest = test[,1]
```

# Summary Statistics

Data table
```{r, echo=FALSE}
table(Ytrain)
table(Ytest)
```

# Pre processing

```{r}
X.train.sca = scale(Xtrain)
```


# Clustering

## 1.PCA

```{r}
library(plyr)
set.seed(1)

pca = princomp(X.train.sca)
```


```{r}
X.pca = pca$scores[,1:25]
k=50
kmeans = kmeans(X.pca, centers =k)

cluster_prediction = rep(NA,k)
#cluster_accuracy = matrix(rep(NA,2*k), nrow = k)
cluster_confidence = matrix(nrow = k,ncol = 2)
clusters = split(Ytrain,kmeans$cluster)
for (i in 1:k) {
  x = as.data.frame(unlist(clusters[i]))
  y = count(x)
  max = which.max(y[,2])
  cluster_prediction[i] = y[max,1]
  s = sum(y[,2])
  cluster_confidence[i,1] = y[max,2]/s
  cluster_confidence[i,2] = s
}

weighted_cluster_confidence =100* (t(cluster_confidence[,1]) %*% cluster_confidence[,2])/sum(cluster_confidence[,2])
cluster_prediction
weighted_cluster_confidence
```


# References





